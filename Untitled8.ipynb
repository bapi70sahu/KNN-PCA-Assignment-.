{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm that is used for both classification and regression problems. It is called a lazy learning algorithm because it does not build a model during training. Instead, it stores the training data and makes predictions when new data is given.\n",
        "\n",
        "KNN works based on the idea that similar data points stay close to each other.\n",
        "\n",
        "Working of KNN:\n",
        "1. Choose the value of K (number of neighbors).\n",
        "2. Calculate the distance between the new data point and all training data points.\n",
        "3. Select the K nearest data points.\n",
        "4. Make prediction based on the nearest neighbors.\n",
        "\n",
        "In classification problems, KNN predicts the class label by using majority voting. The class that appears most frequently among the K nearest neighbors is assigned to the new data point.\n",
        "\n",
        "In regression problems, KNN predicts a continuous value by taking the average of the values of the K nearest neighbors.\n",
        "\n",
        "Thus, KNN works by comparing a new data point with the closest data points and making predictions for both classification and regression tasks.\n"
      ],
      "metadata": {
        "id": "GNwMhBmIALjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "The Curse of Dimensionality is a problem that occurs when the number of features (dimensions) in a dataset becomes very large. As the dimensions increase, the data points become more spread out in the feature space. This makes it difficult for machine learning algorithms to find meaningful patterns.\n",
        "\n",
        "In K-Nearest Neighbors (KNN), the algorithm works by calculating the distance between data points. When the number of dimensions increases, the distance between data points becomes less meaningful because most points appear to be at similar distances from each other.\n",
        "\n",
        "Effects of Curse of Dimensionality on KNN:\n",
        "1. KNN becomes slower because it has to calculate distances in many dimensions.\n",
        "2. The accuracy of KNN decreases because the nearest neighbors may not be truly similar.\n",
        "3. More data is required to maintain good performance as dimensions increase.\n",
        "\n",
        "Because of this problem, KNN performs poorly on high-dimensional data and usually works better when the number of features is small.\n"
      ],
      "metadata": {
        "id": "kNZbrRY5AiSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "Principal Component Analysis (PCA) is an unsupervised machine learning technique used for dimensionality reduction. It transforms the original features into a new set of features called principal components. These principal components are created in such a way that they capture the maximum possible variance (information) from the data.\n",
        "\n",
        "PCA reduces the number of features while keeping most of the important information in the dataset. It does this by creating new features that are combinations of the old features.\n",
        "\n",
        "Difference between PCA and Feature Selection:\n",
        "\n",
        "PCA:\n",
        "1. Creates new features (principal components) from the original features.\n",
        "2. Features are transformed, not selected.\n",
        "3. It is a feature extraction method.\n",
        "\n",
        "Feature Selection:\n",
        "1. Selects a subset of the original features.\n",
        "2. Does not create new features.\n",
        "3. It is a feature selection method.\n",
        "\n",
        "In short, PCA creates new transformed features while feature selection simply chooses the best existing features from the dataset.\n"
      ],
      "metadata": {
        "id": "vAgvc7ZiAq9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "In Principal Component Analysis (PCA), eigenvalues and eigenvectors are mathematical concepts that help in finding the most important directions in the data.\n",
        "\n",
        "Eigenvectors are directions (or axes) along which the data varies the most. They show the new directions in which the data should be projected.\n",
        "\n",
        "Eigenvalues are numbers that tell how much variance (information) is captured by each eigenvector. A larger eigenvalue means that the corresponding eigenvector carries more important information.\n",
        "\n",
        "Importance in PCA:\n",
        "\n",
        "1. Eigenvectors decide the direction of the new principal components.\n",
        "2. Eigenvalues decide the importance of each principal component.\n",
        "3. PCA selects the eigenvectors with the largest eigenvalues to reduce the dimensionality of the dataset while keeping maximum information.\n",
        "\n",
        "Thus, eigenvectors and eigenvalues help PCA identify the most useful features and reduce data size effectively.\n"
      ],
      "metadata": {
        "id": "jvcMZg6qAy0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA) complement each other very well when used together in a single machine learning pipeline.\n",
        "\n",
        "PCA is used first to reduce the number of features (dimensions) in the dataset. It removes noise and keeps only the most important information. This makes the dataset smaller and easier to handle.\n",
        "\n",
        "After PCA, KNN is applied to the reduced dataset. Since KNN works by calculating distances between data points, having fewer features makes the distance calculations faster and more meaningful.\n",
        "\n",
        "How they work together:\n",
        "1. PCA reduces dimensionality and removes irrelevant features.\n",
        "2. The transformed data is then given to KNN.\n",
        "3. KNN performs better because distances are more accurate and computation is faster.\n",
        "\n",
        "Benefits of combining PCA and KNN:\n",
        "- Faster execution of the KNN algorithm.\n",
        "- Improved accuracy by reducing noise.\n",
        "- Better performance on high-dimensional data.\n",
        "\n",
        "Thus, PCA improves the efficiency and performance of KNN when both are used together in a single pipeline.\n"
      ],
      "metadata": {
        "id": "91xz4_HaA5Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: PCA + KNN Pipeline using Wine Dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=2)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "# Train model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liQCOaykBPok",
        "outputId": "53142ace-6bef-45a0-9c6d-6d308a0688bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Train a KNN Classifier on the Wine dataset with and without feature scaling.\n",
        "Compare model accuracy in both cases.\n",
        "\n",
        "Feature scaling is very important for KNN because KNN depends on distance calculation.\n",
        "If features are not scaled, features with large values dominate the distance.\n",
        "\n",
        "In this experiment, we train two KNN models on the Wine dataset:\n",
        "1. Without feature scaling\n",
        "2. With feature scaling using StandardScaler\n",
        "\n",
        "Then we compare the accuracy of both models to see the effect of feature scaling.\n"
      ],
      "metadata": {
        "id": "8_B4WWNRBfx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6: KNN with and without Feature Scaling on Wine Dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------- Model 1: Without Feature Scaling -----------\n",
        "\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "\n",
        "# ----------- Model 2: With Feature Scaling -----------\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy with scaling:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYfazLIrBgp9",
        "outputId": "258a8880-9839-40b4-c0dd-2ec1bc7acba3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "PCA (Principal Component Analysis) is used to reduce the dimensionality of a dataset while keeping most of the important information.\n",
        "The explained variance ratio shows how much variance (information) each principal component captures from the original dataset.\n",
        "\n",
        "In this task, PCA is trained on the Wine dataset and the explained variance ratio for each principal component is printed.\n"
      ],
      "metadata": {
        "id": "BHrMFUfJBjae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: PCA Explained Variance Ratio on Wine Dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "print(pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB_L1WgIBo3q",
        "outputId": "e15023ab-834c-432d-cf60-2bd63f7f1866"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components).\n",
        "Compare the accuracy with the original dataset.\n",
        "\n",
        "In this task, we compare the performance of KNN on:\n",
        "1. The original Wine dataset\n",
        "2. The PCA-transformed dataset using only the top 2 principal components\n",
        "\n",
        "PCA reduces the number of features, which makes the model simpler and faster.\n",
        "By comparing the accuracies, we can see how dimensionality reduction affects model performance.\n"
      ],
      "metadata": {
        "id": "2iqzYX_fBsNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8: KNN on Original Data vs PCA-Reduced Data\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------- KNN on Original Data -----------\n",
        "\n",
        "scaler_original = StandardScaler()\n",
        "X_train_scaled = scaler_original.fit_transform(X_train)\n",
        "X_test_scaled = scaler_original.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "print(\"Accuracy of KNN on Original Dataset:\", accuracy_original)\n",
        "\n",
        "# ----------- PCA Transformation -----------\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# ----------- KNN on PCA Data -----------\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(\"Accuracy of KNN on PCA-Reduced Dataset:\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FFMqs7CB1Mn",
        "outputId": "78f26bd7-3425-4009-9bf4-41e3880e095f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of KNN on Original Dataset: 0.9444444444444444\n",
            "Accuracy of KNN on PCA-Reduced Dataset: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "KNN uses distance metrics to calculate how close data points are to each other.\n",
        "Different distance metrics can affect the performance of the model.\n",
        "\n",
        "In this task, we train two KNN models on the scaled Wine dataset:\n",
        "1. Using Euclidean distance\n",
        "2. Using Manhattan distance\n",
        "\n",
        "Then we compare their accuracies to understand the effect of different distance metrics.\n"
      ],
      "metadata": {
        "id": "3PJKATs8B3hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9: KNN with Euclidean and Manhattan Distance on Scaled Wine Dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ----------- KNN with Euclidean Distance -----------\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "print(\"Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "\n",
        "# ----------- KNN with Manhattan Distance -----------\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"Accuracy with Manhattan distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPy2qokzCHj-",
        "outputId": "f184db0d-c529-4e64-c4af-f1f6d0554968"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9444444444444444\n",
            "Accuracy with Manhattan distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "Answer:\n",
        "\n",
        "Background:\n",
        "High-dimensional gene-expression datasets commonly have thousands of features (genes) but only a small number of samples (patients). This imbalance causes models to overfit, because they can memorize noise. Dimensionality reduction using PCA plus a simple classifier like KNN is a common, interpretable, and effective pipeline when used with careful validation.\n",
        "\n",
        "1) Use PCA to reduce dimensionality\n",
        "- Preprocessing: always standardize the features (e.g., StandardScaler) because PCA and KNN are sensitive to feature scales.\n",
        "- Fit PCA on the training set only (never fit on test data). PCA finds orthogonal directions (principal components) that capture maximal variance.\n",
        "- Transform both training and test data using the PCA fitted on training set.\n",
        "\n",
        "2) Decide how many components to keep\n",
        "- Use explained variance ratio: compute cumulative explained variance and choose the smallest number of components that capture a target percentage of variance (commonly 90–99%). Example: choose the number of components needed to reach 95% cumulative variance.\n",
        "- Practical constraint: retaining 95% variance may still result in many components (and risk overfitting). Also consider:\n",
        "  - Use a small fixed number (e.g., 10–50) chosen by cross-validation performance to balance bias-variance.\n",
        "  - Use a scree plot (look for \"elbow\") to find diminishing returns.\n",
        "  - Use nested cross-validation to select the number of components robustly.\n",
        "- For genomics, prefer a conservative approach: keep enough components to retain biological signal but small enough to avoid overfitting; verify via cross-validation.\n",
        "\n",
        "3) Use KNN for classification post-dimensionality reduction\n",
        "- After PCA transform, train a KNN classifier on the reduced features.\n",
        "- Typical steps:\n",
        "  - Use Stratified train/test split or nested CV (outer loop for final evaluation, inner loop for hyperparameter tuning).\n",
        "  - Tune K (n_neighbors) and distance metric (euclidean, manhattan) using cross-validation on training folds.\n",
        "  - Use StandardScaler before PCA or inside a pipeline.\n",
        "- Rationale: PCA reduces noise and correlation among original gene features; KNN then uses distances in the lower-dimensional, more meaningful subspace.\n",
        "\n",
        "4) Evaluate the model\n",
        "- Use a held-out test set for final evaluation (never used for training or hyperparameter tuning).\n",
        "- Use stratified k-fold cross-validation (or nested CV) on the training set for hyperparameter selection and to estimate generalization performance.\n",
        "- Report multiple metrics: accuracy, precision, recall, F1-score, and confusion matrix (class imbalance matters in biomedical data).\n",
        "- For robust claims, use repeated CV or nested CV and report mean ± std of metrics.\n",
        "- If possible, validate on an independent external cohort (best practice in biomedical studies).\n",
        "\n",
        "5) Justify this pipeline to stakeholders\n",
        "- Simplicity & interpretability: PCA + KNN is easy to explain — PCA reduces features to principal axes, KNN classifies based on similar patients.\n",
        "- Overfitting control: dimensionality reduction reduces parameter space, decreasing chance of overfitting on small samples.\n",
        "- Computational efficiency: PCA reduces storage and compute for downstream models.\n",
        "- Reproducibility & validation: emphasize that we use strict train/test splits and cross-validation, with hyperparameter tuning only inside training folds.\n",
        "- Biological validation: show that components/loaded genes correlate with known biology (if loadings point to gene sets/pathways), and propose follow-up wet-lab validation.\n",
        "- Risk mitigation: communicate limitations (PCA is linear, may miss nonlinear structure) and propose alternatives (nonlinear methods, feature selection by biological priors, regularized models) as needed.\n",
        "\n",
        "Summary:\n",
        "- Pipeline: StandardScaler → PCA (fit on training) → select n_components (via explained variance + CV) → KNN (tune k & metric via CV) → final evaluation on hold-out test set and (if available) independent cohort.\n",
        "- Validate thoroughly and report multiple metrics and confidence intervals; provide biological interpretation of top loadings where possible.\n"
      ],
      "metadata": {
        "id": "CYNhFlD3DRez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q10: PCA + KNN pipeline for high-dimensional gene-expression-like data\n",
        "# This code simulates a gene-expression dataset, demonstrates PCA-based dimensionality reduction,\n",
        "# shows ways to decide number of components, trains KNN after PCA, and evaluates the model.\n",
        "# Paste into Google Colab and run. Outputs will be printed.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# -------------------------\n",
        "# 1) Simulate dataset (replace with real data loading in practice)\n",
        "RANDOM_STATE = 42\n",
        "n_samples = 100         # small sample size (typical in genomics)\n",
        "n_features = 500        # high dimensionality (genes)\n",
        "n_informative = 50      # a small set of informative features\n",
        "n_classes = 3\n",
        "\n",
        "X, y = make_classification(n_samples=n_samples,\n",
        "                           n_features=n_features,\n",
        "                           n_informative=n_informative,\n",
        "                           n_redundant=0,\n",
        "                           n_repeated=0,\n",
        "                           n_classes=n_classes,\n",
        "                           class_sep=1.5,\n",
        "                           random_state=RANDOM_STATE)\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Class counts:\", {i: int(sum(y==i)) for i in np.unique(y)})\n",
        "\n",
        "# -------------------------\n",
        "# 2) Train/test split (final hold-out to evaluate generalization)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
        "print(\"\\nTrain/test shapes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Preprocess: scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)   # fit only on training\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# -------------------------\n",
        "# 4) PCA: fit on training and inspect explained variance\n",
        "# Use randomized SVD for speed on high-dim data\n",
        "pca_full = PCA(svd_solver='randomized', random_state=RANDOM_STATE)\n",
        "pca_full.fit(X_train_scaled)\n",
        "\n",
        "explained = pca_full.explained_variance_ratio_\n",
        "cum_explained = np.cumsum(explained)\n",
        "\n",
        "print(\"\\nExplained variance ratio (first 10):\", np.round(explained[:10], 4))\n",
        "print(\"Cumulative explained variance (first 10):\", np.round(cum_explained[:10], 4))\n",
        "\n",
        "# Decide components: example by threshold (95%) and also a practical small fixed number\n",
        "threshold = 0.95\n",
        "n_components_95 = int(np.searchsorted(cum_explained, threshold) + 1)\n",
        "print(f\"\\nComponents to reach {int(threshold*100)}% variance: {n_components_95}\")\n",
        "\n",
        "# Often we choose a smaller practical number and validate by CV:\n",
        "n_small = 20\n",
        "n_medium = 50\n",
        "n_small = min(n_small, X_train.shape[0]-1)\n",
        "n_medium = min(n_medium, X_train.shape[0]-1)\n",
        "\n",
        "# -------------------------\n",
        "# 5) Transform with PCA (trained on training set)\n",
        "pca_small = PCA(n_components=n_small, svd_solver='randomized', random_state=RANDOM_STATE)\n",
        "pca_med   = PCA(n_components=n_medium, svd_solver='randomized', random_state=RANDOM_STATE)\n",
        "\n",
        "X_train_pca_small = pca_small.fit_transform(X_train_scaled)\n",
        "X_test_pca_small  = pca_small.transform(X_test_scaled)\n",
        "\n",
        "X_train_pca_med = pca_med.fit_transform(X_train_scaled)\n",
        "X_test_pca_med  = pca_med.transform(X_test_scaled)\n",
        "\n",
        "print(\"\\nShapes after PCA (small):\", X_train_pca_small.shape, X_test_pca_small.shape)\n",
        "print(\"Shapes after PCA (medium):\", X_train_pca_med.shape, X_test_pca_med.shape)\n",
        "\n",
        "# -------------------------\n",
        "# 6) KNN training and quick CV-based comparison of k and metric\n",
        "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_STATE)\n",
        "ks = [3, 5, 7]\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "\n",
        "def evaluate_options(X_tr, y_tr, X_te, y_te, label):\n",
        "    print(f\"\\nEvaluating K options on: {label}\")\n",
        "    results = []\n",
        "    for k in ks:\n",
        "        for m in metrics:\n",
        "            knn = KNeighborsClassifier(n_neighbors=k, metric=m)\n",
        "            cv_scores = cross_val_score(knn, X_tr, y_tr, cv=cv, scoring='accuracy', n_jobs=1)\n",
        "            mean_cv = cv_scores.mean()\n",
        "            # Fit on full training and evaluate on hold-out test\n",
        "            knn.fit(X_tr, y_tr)\n",
        "            y_pred = knn.predict(X_te)\n",
        "            test_acc = accuracy_score(y_te, y_pred)\n",
        "            print(f\"k={k}, metric={m} -> CV acc: {mean_cv:.4f}, Test acc: {test_acc:.4f}\")\n",
        "            results.append((mean_cv, test_acc, k, m))\n",
        "    results.sort(reverse=True, key=lambda x: x[0])  # sort by CV acc\n",
        "    best = results[0]\n",
        "    print(\"Best (by CV): CV_acc={:.4f}, Test_acc={:.4f}, k={}, metric={}\".format(best[0], best[1], best[2], best[3]))\n",
        "    return best\n",
        "\n",
        "best_small = evaluate_options(X_train_pca_small, y_train, X_test_pca_small, y_test, \"PCA small ({} comps)\".format(n_small))\n",
        "best_med   = evaluate_options(X_train_pca_med, y_train, X_test_pca_med, y_test, \"PCA medium ({} comps)\".format(n_medium))\n",
        "\n",
        "# Also evaluate on original scaled data as baseline (no PCA)\n",
        "print(\"\\nBaseline: KNN on original scaled data\")\n",
        "baseline_best = evaluate_options(X_train_scaled, y_train, X_test_scaled, y_test, \"Original scaled data\")\n",
        "\n",
        "# -------------------------\n",
        "# 7) Final evaluation: pick whichever configuration has best CV performance (as example)\n",
        "# Here we demonstrate final test evaluation using 'best_small' config on PCA-small\n",
        "best_k, best_metric = int(best_small[2]), best_small[3]\n",
        "final_knn = KNeighborsClassifier(n_neighbors=best_k, metric=best_metric)\n",
        "final_knn.fit(X_train_pca_small, y_train)\n",
        "y_final = final_knn.predict(X_test_pca_small)\n",
        "\n",
        "print(\"\\nFinal evaluation on hold-out test (PCA small):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_final))\n",
        "print(\"Classification report:\\n\", classification_report(y_test, y_final, digits=4))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_final))\n",
        "\n",
        "# -------------------------\n",
        "# Notes for real data:\n",
        "# - Replace the simulated dataset with real gene-expression matrix and labels.\n",
        "# - Consider nested CV for hyperparameter tuning and an external validation cohort if possible.\n",
        "# - For interpretability, examine PCA components' loadings and check if top genes map to known pathways.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cjpdCftD1Cs",
        "outputId": "bfe517c5-69f6-4f4c-9f9c-440876a01163"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (100, 500)\n",
            "Class counts: {np.int64(0): 34, np.int64(1): 33, np.int64(2): 33}\n",
            "\n",
            "Train/test shapes: (80, 500) (20, 500)\n",
            "\n",
            "Explained variance ratio (first 10): [0.0254 0.023  0.0221 0.0214 0.0212 0.0209 0.0208 0.0203 0.0198 0.0194]\n",
            "Cumulative explained variance (first 10): [0.0254 0.0484 0.0705 0.0918 0.113  0.1339 0.1547 0.175  0.1948 0.2142]\n",
            "\n",
            "Components to reach 95% variance: 71\n",
            "\n",
            "Shapes after PCA (small): (80, 20) (20, 20)\n",
            "Shapes after PCA (medium): (80, 50) (20, 50)\n",
            "\n",
            "Evaluating K options on: PCA small (20 comps)\n",
            "k=3, metric=euclidean -> CV acc: 0.3875, Test acc: 0.6000\n",
            "k=3, metric=manhattan -> CV acc: 0.4625, Test acc: 0.3500\n",
            "k=5, metric=euclidean -> CV acc: 0.5375, Test acc: 0.5000\n",
            "k=5, metric=manhattan -> CV acc: 0.4750, Test acc: 0.6500\n",
            "k=7, metric=euclidean -> CV acc: 0.4500, Test acc: 0.4500\n",
            "k=7, metric=manhattan -> CV acc: 0.4500, Test acc: 0.5000\n",
            "Best (by CV): CV_acc=0.5375, Test_acc=0.5000, k=5, metric=euclidean\n",
            "\n",
            "Evaluating K options on: PCA medium (50 comps)\n",
            "k=3, metric=euclidean -> CV acc: 0.3625, Test acc: 0.4500\n",
            "k=3, metric=manhattan -> CV acc: 0.4125, Test acc: 0.4500\n",
            "k=5, metric=euclidean -> CV acc: 0.4750, Test acc: 0.6000\n",
            "k=5, metric=manhattan -> CV acc: 0.4875, Test acc: 0.3500\n",
            "k=7, metric=euclidean -> CV acc: 0.4000, Test acc: 0.5500\n",
            "k=7, metric=manhattan -> CV acc: 0.4250, Test acc: 0.4000\n",
            "Best (by CV): CV_acc=0.4875, Test_acc=0.3500, k=5, metric=manhattan\n",
            "\n",
            "Baseline: KNN on original scaled data\n",
            "\n",
            "Evaluating K options on: Original scaled data\n",
            "k=3, metric=euclidean -> CV acc: 0.4000, Test acc: 0.3500\n",
            "k=3, metric=manhattan -> CV acc: 0.5000, Test acc: 0.4000\n",
            "k=5, metric=euclidean -> CV acc: 0.5125, Test acc: 0.4000\n",
            "k=5, metric=manhattan -> CV acc: 0.4875, Test acc: 0.5500\n",
            "k=7, metric=euclidean -> CV acc: 0.5125, Test acc: 0.4500\n",
            "k=7, metric=manhattan -> CV acc: 0.5750, Test acc: 0.5000\n",
            "Best (by CV): CV_acc=0.5750, Test_acc=0.5000, k=7, metric=manhattan\n",
            "\n",
            "Final evaluation on hold-out test (PCA small):\n",
            "Accuracy: 0.5\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5000    0.7143    0.5882         7\n",
            "           1     0.5000    0.4286    0.4615         7\n",
            "           2     0.5000    0.3333    0.4000         6\n",
            "\n",
            "    accuracy                         0.5000        20\n",
            "   macro avg     0.5000    0.4921    0.4833        20\n",
            "weighted avg     0.5000    0.5000    0.4874        20\n",
            "\n",
            "Confusion matrix:\n",
            " [[5 1 1]\n",
            " [3 3 1]\n",
            " [2 2 2]]\n"
          ]
        }
      ]
    }
  ]
}